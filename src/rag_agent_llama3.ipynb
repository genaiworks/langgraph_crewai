{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Using cached langchain_nomic-0.0.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain_community in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.0.14)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.5.2)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.1.4)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.1.17-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langgraph in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.0.15)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.0.48-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tavily-python in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.3.1)\n",
      "Collecting tavily-python\n",
      "  Using cached tavily_python-0.3.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gpt4all\n",
      "  Using cached gpt4all-2.6.0-py3-none-macosx_10_15_universal2.whl.metadata (4.1 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement firecreawl-py (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for firecreawl-py\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecreawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting firecrawl-py\n",
      "  Downloading firecrawl_py-0.0.6-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from firecrawl-py) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests->firecrawl-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests->firecrawl-py) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests->firecrawl-py) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests->firecrawl-py) (2024.2.2)\n",
      "Downloading firecrawl_py-0.0.6-py3-none-any.whl (2.6 kB)\n",
      "Installing collected packages: firecrawl-py\n",
      "Successfully installed firecrawl-py-0.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm ='llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm='llama3:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Using cached langchain_nomic-0.0.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: langchain_community in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.0.14)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.5.2)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: firecrawl-py in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.0.6)\n",
      "Collecting firecrawl-py\n",
      "  Downloading firecrawl_py-0.0.8-py3-none-any.whl.metadata (221 bytes)\n",
      "Collecting gpt4all\n",
      "  Using cached gpt4all-2.6.0-py3-none-macosx_10_15_universal2.whl.metadata (4.1 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.1.4)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.19-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langgraph in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.0.15)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.0.48-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tavily-python in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (0.3.1)\n",
      "Collecting tavily-python\n",
      "  Using cached tavily_python-0.3.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain-nomic) (0.1.23)\n",
      "Collecting nomic<4.0.0,>=3.0.12 (from langchain-nomic)\n",
      "  Downloading nomic-3.0.27.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (0.6.5)\n",
      "Collecting langchain-core<0.2,>=0.1 (from langchain-nomic)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.56-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from gpt4all) (4.66.4)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from chromadb) (2.7.1)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from chromadb) (4.11.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.17.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from chromadb) (1.24.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from chromadb) (1.24.0)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.63.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.2 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.1.3-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from chromadb) (0.9.4)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.3-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (24.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading fastapi_cli-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Collecting jinja2>=2.11.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading ujson-5.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1->langchain-nomic) (1.33)\n",
      "Collecting packaging>=19.1 (from build>=1.0.3->chromadb)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (8.1.7)\n",
      "Collecting jsonlines (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from nomic<4.0.0,>=3.0.12->langchain-nomic) (13.7.1)\n",
      "Collecting pandas (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting pyarrow (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading pyarrow-16.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pillow (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyjwt (from nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.24.0)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.45b0)\n",
      "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.5.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from pydantic>=1.9->chromadb) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from httpx>=0.23.0->fastapi>=0.95.2->chromadb) (1.3.1)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-nomic) (2.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (2.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nomic<4.0.0,>=3.0.12->langchain-nomic)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.12->langchain-nomic) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Downloading langchain_nomic-0.0.2-py3-none-any.whl (3.4 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading firecrawl_py-0.0.8-py3-none-any.whl (3.1 kB)\n",
      "Downloading gpt4all-2.6.0-py3-none-macosx_10_15_universal2.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
      "Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.7/198.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-0.0.48-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
      "Downloading bcrypt-4.1.3-cp39-abi3-macosx_10_12_universal2.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.5/506.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.63.0-cp311-cp311-macosx_10_9_universal2.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.56-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading onnxruntime-1.17.3-cp311-cp311-macosx_11_0_universal2.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading orjson-3.10.3-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (253 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.6/253.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
      "Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Downloading fastapi_cli-0.0.3-py3-none-any.whl (9.2 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl (145 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.9/145.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "Using cached importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ujson-5.9.0-cp311-cp311-macosx_11_0_arm64.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.2/418.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp311-cp311-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: nomic, pypika\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.0.27-py3-none-any.whl size=44143 sha256=00db69c6fbcd839aef52ba307d930a5bb6dc93a0df139eaa221aec288ab48a8b\n",
      "  Stored in directory: /Users/genai/Library/Caches/pip/wheels/1a/fe/4c/4e7210dc450ee4a764878e49f36c5ab49028498c9eae0be2dd\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=f7f5856fe332325a016c1a4f818364e2e2dba1078af367c29d79445b68f70619\n",
      "  Stored in directory: /Users/genai/Library/Caches/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built nomic pypika\n",
      "Installing collected packages: pytz, pypika, mpmath, monotonic, mmh3, flatbuffers, websockets, websocket-client, uvloop, uvicorn, ujson, tzdata, types-requests, sympy, shellingham, python-multipart, pyproject_hooks, pyjwt, pyarrow, pillow, packaging, overrides, orjson, opentelemetry-util-http, MarkupSafe, loguru, jsonlines, importlib-resources, importlib-metadata, humanfriendly, httptools, grpcio, fsspec, filelock, dnspython, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, tiktoken, starlette, posthog, pandas, langchainhub, jinja2, huggingface-hub, gpt4all, firecrawl-py, email_validator, coloredlogs, build, typer, tokenizers, tavily-python, opentelemetry-instrumentation, onnxruntime, nomic, langsmith, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, opentelemetry-instrumentation-fastapi, langgraph, langchain-text-splitters, langchain-nomic, langchain_community, langchain, fastapi-cli, fastapi, chromadb\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 7.1.0\n",
      "    Uninstalling importlib_metadata-7.1.0:\n",
      "      Successfully uninstalled importlib_metadata-7.1.0\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.2\n",
      "    Uninstalling tiktoken-0.5.2:\n",
      "      Successfully uninstalled tiktoken-0.5.2\n",
      "  Attempting uninstall: firecrawl-py\n",
      "    Found existing installation: firecrawl-py 0.0.6\n",
      "    Uninstalling firecrawl-py-0.0.6:\n",
      "      Successfully uninstalled firecrawl-py-0.0.6\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.4\n",
      "    Uninstalling typer-0.9.4:\n",
      "      Successfully uninstalled typer-0.9.4\n",
      "  Attempting uninstall: tavily-python\n",
      "    Found existing installation: tavily-python 0.3.1\n",
      "    Uninstalling tavily-python-0.3.1:\n",
      "      Successfully uninstalled tavily-python-0.3.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.23\n",
      "    Uninstalling langchain-core-0.1.23:\n",
      "      Successfully uninstalled langchain-core-0.1.23\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.0.15\n",
      "    Uninstalling langgraph-0.0.15:\n",
      "      Successfully uninstalled langgraph-0.0.15\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.0.14\n",
      "    Uninstalling langchain-community-0.0.14:\n",
      "      Successfully uninstalled langchain-community-0.0.14\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.4\n",
      "    Uninstalling langchain-0.1.4:\n",
      "      Successfully uninstalled langchain-0.1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "instructor 0.5.2 requires typer<0.10.0,>=0.9.0, but you have typer 0.12.3 which is incompatible.\n",
      "langchain-openai 0.0.2.post1 requires tiktoken<0.6.0,>=0.5.2, but you have tiktoken 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.3 filelock-3.14.0 firecrawl-py-0.0.8 flatbuffers-24.3.25 fsspec-2024.3.1 gpt4all-2.6.0 grpcio-1.63.0 httptools-0.6.1 huggingface-hub-0.23.0 humanfriendly-10.0 importlib-metadata-7.0.0 importlib-resources-6.4.0 jinja2-3.1.4 jsonlines-4.0.0 kubernetes-29.0.0 langchain-0.1.19 langchain-core-0.1.52 langchain-nomic-0.0.2 langchain-text-splitters-0.0.1 langchain_community-0.0.38 langchainhub-0.1.15 langgraph-0.0.48 langsmith-0.1.56 loguru-0.7.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 nomic-3.0.27 onnxruntime-1.17.3 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.3 overrides-7.7.0 packaging-23.2 pandas-2.2.2 pillow-10.3.0 posthog-3.5.0 pyarrow-16.0.0 pyjwt-2.8.0 pypika-0.48.9 pyproject_hooks-1.1.0 python-multipart-0.0.9 pytz-2024.1 shellingham-1.5.4 starlette-0.37.2 sympy-1.12 tavily-python-0.3.3 tiktoken-0.6.0 tokenizers-0.19.1 typer-0.12.3 types-requests-2.31.0.20240406 tzdata-2024.1 ujson-5.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websocket-client-1.8.0 websockets-12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-nomic langchain_community tiktoken firecrawl-py gpt4all langchainhub chromadb langchain langgraph tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls =[\n",
    "\"https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost\",\n",
    "\"https://www.ai-jason.com/learning-ai/gpt5-llm\",\n",
    "\"https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [FireCrawlLoader(api_key=\"fc-9d9edbfedf674d2b8cedc83199db0192\", url=url, mode=\"scrape\").load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the documents into chunks\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "_result = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        _result.append(item)\n",
    "# That's really all it is, except that _result doesn't have a name and is appended to implicitly. \n",
    "# The outer loop iterates over the three lists of l and for each of those lists, \n",
    "# the inner loop iterates over the items in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nAre you building AI agents or using chatGPT? If so, you may be facing the challenge of high costs associated with large language models (LLM). In this article, we will explore effective strategies to reduce LLM costs by up to 78%. Let\\'s dive in!\\n\\n\\u200d\\n\\n1\\\\. Change Model\\n----------------\\n\\nOne effective way to reduce LLM costs is to change the model you are using. Different models have different costs associated with them. For example, GPT-4 is the most powerful but also the most expensive model, while Mistro 7B is significantly cheaper. By using a smaller model for specific tasks and reserving the more expensive model for complex questions, you can achieve significant cost savings.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg)\\n\\n2\\\\. Large Language Model Router\\n-------------------------------\\n\\nThe concept of a large language model router involves using a cascade of models to handle different types of questions. Cheaper models are used first, and if they are unable to provide a satisfactory answer, the question is passed on to a more expensive model. This approach leverages the significant cost difference between models and can result in substantial cost savings.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedae566010ad14fe382cd_llm%20router.jpg)\\n\\n3\\\\. Multi-Agent Setup\\n---------------------\\n\\nAnother strategy is to set up multiple agents, each using a different model. The first agent attempts to complete the task using a cheaper model, and if it fails, the next agent is invoked. By using this multi-agent setup, you can achieve similar or even better success rates while significantly reducing costs.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaefa7461fca4f82df51_autogen.jpg)\\n\\n4\\\\. LLM Lingua\\n--------------\\n\\nLLM Lingua is a method introduced by Microsoft that focuses on optimizing the input and output of large language models. By removing unnecessary tokens and words from the input, you can significantly reduce the cost of running the model. This method is particularly effective for tasks such as summarization or answering specific questions based on a transcript.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)\\n\\n5\\\\. Optimize Agent Memory\\n-------------------------\\n\\nOptimizing agent memory is another way to reduce LLM costs. By carefully managing the amount of conversation history stored in memory, you can minimize the number of tokens required for each interaction. This can lead to significant cost savings, especially when dealing with long conversations.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedb03355b5d49f9ac7f8e_memory%20opt.jpg)\\n\\n6\\\\. Observability\\n-----------------\\n\\nHaving a deep understanding of the cost patterns in your LLM application is crucial for effective cost optimization. By using observability platforms like L Smith, you can monitor and log the cost for each large language model. This allows you to identify areas where costs can be optimized and make informed decisions to reduce overall expenses.\\n\\nBy implementing these strategies, you can reduce LLM costs by up to 78% or more. Remember, reducing costs while maintaining performance and user experience is a critical skill for AI startups. Stay proactive and continuously optimize your LLM usage to maximize efficiency and profitability.\\n\\n\\u200d\\n\\nGet free HubSpot AI For Marketers Course: [https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\n\\n🔗 Links\\n\\n*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n*   Join my AI email list: [https://crafters.ai/](https://crafters.ai/)\\n    \\n*   My discord: [https://discord.gg/eZXprSaCDE](https://discord.gg/eZXprSaCDE)\\n    \\n*   Inbox Agent: [https://www.youtube.com/watch?v=Jv\\\\_e6Rt4vWE&t=23s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=Jv_e6Rt4vWE&t=23s&ab_channel=AIJason)\\n    \\n*   Research Agent: [https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab_channel=AIJason)\\n    \\n*   James Brigg on Agent Memory: [https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)\\n    \\n*   Another video about details for LLM cost tracking: [https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab\\\\_channel=LearnfromOpenSourcewithElie](https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab_channel=LearnfromOpenSourcewithElie)\\n    \\n\\n\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### Q: How can I determine which model is the most cost-effective for my AI application?\\n\\nA: To determine the most cost-effective model for your AI application, you should consider the specific tasks and requirements of your application. Evaluate the performance and cost trade-offs of different models and choose the one that best fits your needs.\\n\\n### Q: Are there any open-source solutions available for large language model routing?\\n\\nA: While there are no specific open-source solutions for large language model routing, you can explore frameworks like Hugging Face\\'s Hugging GPT, which allows you to build your own routing logic using a large language model as a controller.\\n\\n### Q: How often should I monitor and optimize my LLM costs?\\n\\nA: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[The REAL cost of LLM (And How to reduce 78%+ of Cost)](https://www.youtube.com/watch?v=lHxl5SchjPA)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=lHxl5SchjPA \"Watch on YouTube\")', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nAI has come a long way in recent years, with large language models (LLMs) like GPT-4 impressing us with their ability to generate text. However, these models primarily rely on system one thinking, which is fast and intuitive but lacks the ability to break down complex problems into smaller steps and explore different options. This limitation has led researchers to focus on developing GPT-5 with enhanced reasoning abilities and reliability.\\n\\nThe Two Modes of Thinking\\n-------------------------\\n\\nIn his book \"Thinking, Fast and Slow,\" Daniel Kahneman introduces the concept of two modes of thinking: system one and system two. System one thinking is our fast, intuitive brain that quickly provides answers based on memorized information. On the other hand, system two thinking is slower but more rational, requiring us to take time, calculate, and analyze before arriving at an answer.\\n\\nSimilarly, large language models like GPT-4 primarily rely on system one thinking. They predict the best next words based on the sequence of words they have seen before, without truly understanding the complex problems they are trying to solve.\\n\\n![](https://assets-global.website-files.com/img/image-placeholder.svg)\\n\\n\\u200d\\n\\nThe Limitations of GPT-4\\n------------------------\\n\\nGPT-4, despite its impressive capabilities, lacks system two thinking. It cannot break down complex tasks into smaller steps or explore different options. It simply generates text based on patterns it has learned from training data. This limitation becomes evident when GPT-4 is faced with complex problems that require deeper analysis and reasoning.\\n\\nFor example, in a video by Veritasium, college students were asked seemingly simple questions like the time it takes for the Earth to go around the Sun. Many of them answered incorrectly because they relied on system one thinking, providing automatic intuitive answers without truly considering the question.\\n\\nLarge language models like GPT-4 face a similar challenge. They lack the ability to think critically and break down complex problems into smaller, manageable steps. This is where GPT-5 comes in.\\n\\nThe Promise of GPT-5\\n--------------------\\n\\nGPT-5 aims to enhance the reasoning abilities of large language models and introduce system two thinking. OpenAI\\'s Sam Altman mentioned in an interview with Bill Gates that the key milestones for GPT-5 will be around reasoning ability and reliability.\\n\\nCurrently, GPT-4 can reason in extremely limited ways and lacks reliability. It may provide correct answers, but it doesn\\'t always know which answer is the best. GPT-5 aims to improve this by increasing reliability and enhancing reasoning abilities.\\n\\nAltman also mentioned the possibility of GPT-5 being able to solve complex math equations by applying transformations an arbitrary number of times. This would require a more complex control logic for reasoning, going beyond what is currently possible with GPT-4.\\n\\nHowever, simply improving the model itself is not enough. There are ways to enforce system two thinking in large language models today, even with GPT-4.\\n\\nPromoting System 2 Thinking in Large Language Models\\n----------------------------------------------------\\n\\nThere are two common strategies to promote system two thinking in large language models: prompt engineering and communicative agents.\\n\\n### Prompt Engineering\\n\\nPrompt engineering is a simple and common method to guide large language models towards system two thinking. One approach is the \"chain of thought,\" where a sentence is inserted step by step before the model generates any text. This forces the model to break down the problem into smaller steps and think through each one.\\n\\nAnother approach is to provide a few short prompt examples instead of a step-by-step process. These examples guide the model towards thinking through different steps and considering multiple possibilities.\\n\\nWhile prompt engineering can be effective in promoting system two thinking, it has limitations. It often restricts the model to consider only one possibility and may not explore diverse options, similar to how humans approach creative problem-solving.\\n\\nTo address this limitation, more advanced prompting tactics like self-consistency with chain of thought (SCCOT) have been proposed. SCCOT involves running the chain of thought process multiple times and reviewing and voting on the most reasonable answers. This allows for some exploration of different options but requires more implementation effort.\\n\\nAnother advanced prompting tactic is the tree of sorts, which simulates a tree search to explore different options and paths. It keeps track of all the paths explored and allows for backtracking if the current path doesn\\'t lead to the desired outcome. However, implementing the tree of sorts is complex and requires significant implementation effort.\\n\\n### Communicative Agents\\n\\nCommunicative agents provide an elegant solution to promote system two thinking in large language models. These are multi-agent setups where users can define different agents and simulate conversations between them. The agents can reflect and spot flaws in each other\\'s perspectives and thinking processes.\\n\\nCommunicative agents have shown promise in enhancing system two thinking. They allow for dedicated agents to review and critique the model\\'s answers, identifying flaws and providing feedback. This collaborative approach mimics how humans solve complex problems by exploring multiple options and learning from each other.\\n\\nSetting up communicative agents can be done using various frameworks like ChatGPT, MetaGPT, Autogen, and Crew AI. These frameworks enable the creation of agent workflows and facilitate conversations between agents with different roles, such as problem solvers and reviewers.\\n\\nAutogen Studio, a no-code interface for Autogen, simplifies the setup of communicative agent workflows. It allows for easy collaboration and problem-solving between agents, making it accessible to a wider range of users.\\n\\nUnlocking System 2 Thinking with GPT-4 Today\\n--------------------------------------------\\n\\nWhile GPT-4 may not have native system two thinking capabilities, prompt engineering and communicative agents can be used to enforce system two thinking and solve complex tasks.\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides the model towards thinking through problems step by step and considering multiple possibilities. However, prompt engineering may limit exploration and diversity of solutions.\\n\\nCommunicative agents, on the other hand, provide a collaborative approach to problem-solving. By simulating conversations between agents, users can leverage the strengths of system one and system two thinking. Reviewers can spot flaws in the model\\'s answers, while problem solvers can iterate and improve their solutions based on feedback.\\n\\nFrameworks like Autogen Studio make it easy to set up communicative agent workflows, allowing for seamless collaboration and problem-solving.\\n\\nThe Future of GPT-5 and System 2 Thinking\\n-----------------------------------------\\n\\nGPT-5 holds the promise of unlocking system two thinking in large language models. With enhanced reasoning abilities and reliability, GPT-5 aims to bridge the gap between system one and system two thinking, enabling models to solve complex problems more effectively.\\n\\nResearchers are actively working on developing GPT-5 with improved reasoning abilities. The focus is on enabling large language models to break down complex tasks, explore different options, and make more accurate and informed decisions.\\n\\nAs we look forward to the advancements in GPT-5, it\\'s important to continue exploring and implementing strategies like prompt engineering and communicative agents to drive system two thinking in large language models today.\\n\\n\\u200d\\n\\nFollow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n\\n\\u200d\\n\\nFAQs\\n----\\n\\n### 1\\\\. Can GPT-4 solve complex problems?\\n\\nGPT-4 can generate text and provide answers, but it primarily relies on system one thinking. It lacks the ability to break down complex problems into smaller steps and explore different options.\\n\\n### 2\\\\. How can prompt engineering promote system two thinking?\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides large language models towards thinking through problems step by step and considering multiple possibilities.\\n\\n### 3\\\\. What are communicative agents?\\n\\nCommunicative agents are multi-agent setups where users can define different agents and simulate conversations between them. This allows for collaborative problem-solving and promotes system two thinking.\\n\\n### 4\\\\. How can communicative agents be set up?\\n\\nFrameworks like Autogen Studio provide a no-code interface for setting up communicative agent workflows. Users can define agents, assign roles, and simulate conversations to solve complex problems.\\n\\n### 5\\\\. What is the future of GPT-5?\\n\\nGPT-5 aims to enhance reasoning abilities and bridge the gap between system one and system two thinking. It holds the promise of enabling large language models to solve complex problems more effectively.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[GPT5 unlocks LLM System 2 Thinking?](https://www.youtube.com/watch?v=sD0X-lWPdxg)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=sD0X-lWPdxg \"Watch on YouTube\")', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to Build Agent workforce Tutorial- AI agent manages community 24/7\\n======================================================================\\n\\nAI agent manages community 24/7 - Build Agent workforce\\n=======================================================\\n\\nAre you overwhelmed with the amount of emails, messages, and tasks you need to handle every day? Do you wish you had a personal assistant to help you manage your workload? Well, thanks to advancements in artificial intelligence (AI), you can now build your own AI employees to take care of these tasks for you. In this series of videos, I will show you how to create AI agents that can act as your digital workforce, handling various roles and tasks.\\n\\nWhy Build AI Employees?\\n-----------------------\\n\\nAs a content creator with a growing YouTube channel, I found myself struggling to keep up with the influx of emails, messages from various platforms, and the need to stay on top of new purchases and video editing work. I considered outsourcing some of the work or hiring someone to help me, but then I had an idea. Instead of relying on others, why not build AI employees to handle these tasks? This would not only be an interesting experiment but also a way to push the boundaries of AI technology.\\n\\nBuilding AI employees is similar to building self-driving cars. While many researchers are focused on developing fully autonomous AI agents (level 5), there are plenty of opportunities to create AI agents that require some human guidance (level 3 or level 2). These AI agents can still deliver excellent results and are more feasible to build at this stage. My goal is not to create fully autonomous agents but to build AI employees that can get the work done with some guidance.\\n\\nSo, the first AI employee I want to build is a community moderator for my Discord community. As a content creator based in Australia, I have community members from different parts of the world with wildly different time zones. It\\'s challenging for me to respond to messages in a timely manner. Ideally, I would love to have a community moderator who can proactively engage with people 24/7. This AI moderator should have deep domain knowledge in AI and software development, stay updated on AI trends, and engage with the community proactively.\\n\\nChallenges to Overcome\\n----------------------\\n\\nBuilding an AI community moderator comes with its own set of challenges. Firstly, how should the AI behave in a group chat setup? Most AI agents are designed for one-on-one conversations, but in a Discord community, there can be multiple people talking at the same time. Secondly, the AI moderator should have a long-term memory of each community member to make conversations feel more genuine and to connect community members with similar interests. Lastly, the AI moderator should be able to retrieve knowledge from various sources, such as my YouTube channel, GitHub repository, and the internet, to provide up-to-date information.\\n\\nTo overcome these challenges, I found an open-source project called Discord AI chatbot on GitHub. This project integrates OpenAI into a Discord bot, taking care of the network setup and providing design patterns for AI behavior in a group chat setup. It allows me to define trigger words that activate the AI moderator and retrieve the relevant conversation history. While this project has some limitations, such as limited conversation history and the inability to perform complex tasks, it provides a flexible foundation for customization.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee0a5f2e303fd2cefe0b3_Copy%20of%20Your%20paragraph%20text.jpg)\\n\\nGetting Started: Installing Discord AI Chatbot\\n----------------------------------------------\\n\\nTo get started, you need to install the Discord AI chatbot. Once the AI chatbot is up and running, you can customize its behavior by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.\\n\\nEnhancing the AI Moderator: Building an AI Agent\\n------------------------------------------------\\n\\nWhile the AI chatbot provides a good starting point, I wanted to create an AI moderator with more advanced capabilities. I wanted the AI moderator to have a long-term memory, retrieve knowledge from various sources, and schedule tasks. To achieve this, I used the Launching Agent framework and integrated it into the AI chatbot.\\n\\nFirstly, I created a knowledge retrieval function to extract information from my YouTube channel, GitHub repository, and the internet. I used a platform called Random CI to turn my website into a knowledge base and used the Launching Agent\\'s large language model to perform knowledge retrieval. I also implemented functions for online research, such as Google search and website scripting, to gather information from the internet.\\n\\nNext, I created a research agent that combines the knowledge retrieval function and online research tools. This research agent can search my internal knowledge base first and then perform online research if necessary. It can provide relevant information and links to answer questions from the community.\\n\\nTo integrate the research agent into the AI chatbot, I modified the AI utility file. I updated the message handling function to pass the user\\'s message to the research agent and added a mapping of agents for each user. This allows each user to have a unique chat history and memory. I also added a scheduler feature to trigger the research agent to generate GitHub reports every two days.\\n\\nGenerating GitHub Reports: Sharing AI Trends and News\\n-----------------------------------------------------\\n\\nIn addition to moderating the community, I wanted the AI agent to share the latest AI trends and news from various sources, such as Twitter, GitHub, and newsletters. For the initial MVP, I decided to generate GitHub reports every two days. The AI agent would check the GitHub trending page, filter out projects related to generative AI, and write a report to keep the community updated.\\n\\nTo achieve this, I used a platform called BrowseAI to extract structured data from the GitHub trending page. BrowseAI allows me to define the elements I want to extract, such as the name of the repo, the link, and the number of stars. I integrated BrowseAI into the AI agent to script the GitHub trending page and retrieve the relevant information.\\n\\nI also implemented a scheduler feature in the AI chatbot to trigger the generation of GitHub reports every two days. This feature allows the AI agent to automatically post the reports in the Discord channels that have enabled the schedule task.\\n\\nDeployment and Continuous Operation\\n-----------------------------------\\n\\nTo ensure the AI moderator operates 24/7, you can deploy the AI chatbot on a cloud service. One option is to use Render, a platform that allows you to deploy web services. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nHowever, keep in mind that free versions of cloud services may have limitations, such as inactivity timeouts. To prevent the AI moderator from going inactive, you can use uptime monitoring services like Uptime Robot to ping your API endpoint at regular intervals.\\n\\nWith your AI moderator deployed and continuously operational, you can enjoy the benefits of having a 24/7 community manager. The AI moderator will handle messages, answer questions, and share the latest AI trends and news, allowing you to focus on other aspects of your content creation.\\n\\nConclusion\\n----------\\n\\nBuilding AI employees, such as a community moderator, can greatly enhance your productivity and efficiency. With the right tools and frameworks, you can create AI agents that handle various tasks and roles, providing a seamless experience for your community members. By combining knowledge retrieval, online research, and scheduling capabilities, you can build an AI moderator that stays engaged with your community 24/7.\\n\\n🔗 Links\\n\\n*   Github repo: [https://github.com/JayZeeDesign/Discord-AI-Chatbot](https://github.com/JayZeeDesign/Discord-AI-Chatbot)\\n    \\n*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n\\n\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### 1\\\\. Can I customize the behavior of the AI moderator?\\n\\nYes, you can customize the behavior of the AI moderator by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.\\n\\n### 2\\\\. Can the AI moderator handle multiple conversations in a group chat setup?\\n\\nYes, the AI moderator is designed to handle group chat setups. It can understand the context of the conversation and respond accordingly, even when multiple people are talking at the same time.\\n\\n### 3\\\\. How does the AI moderator retrieve knowledge from different sources?\\n\\nThe AI moderator can retrieve knowledge from various sources, such as your YouTube channel, GitHub repository, and the internet. It uses a combination of knowledge retrieval techniques and online research tools to gather relevant information.\\n\\n### 4\\\\. Can the AI moderator generate reports and share AI trends?\\n\\nYes, the AI moderator can generate reports and share the latest AI trends. It can retrieve information from GitHub, filter out projects related to generative AI, and write reports to keep the community updated.\\n\\n### 5\\\\. How can I deploy the AI moderator on a cloud service?\\n\\nTo deploy the AI moderator on a cloud service, you can use platforms like Render. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[AI agent manages community 24/7 - Build Agent workforce ep#1](https://www.youtube.com/watch?v=yhBiVrigWNI)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=yhBiVrigWNI \"Watch on YouTube\")', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'})]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nAre you building AI agents or using chatGPT? If so, you may be facing the challenge of high costs associated with large language models (LLM). In this article, we will explore effective strategies to reduce LLM costs by up to 78%. Let\\'s dive in!\\n\\n\\u200d\\n\\n1\\\\. Change Model\\n----------------\\n\\nOne effective way to reduce LLM costs is to change the model you are using. Different models have different costs associated with them. For example, GPT-4 is the most powerful but also the most expensive model, while Mistro 7B is significantly cheaper. By using a smaller model for specific tasks and reserving the more expensive model for complex questions, you can achieve significant cost savings.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg)\\n\\n2\\\\. Large Language Model Router\\n-------------------------------\\n\\nThe concept of a large language model router involves using a cascade of models to handle different types of questions. Cheaper models are used first, and if they are unable to provide a satisfactory answer, the question is passed on to a more expensive model. This approach leverages the significant cost difference between models and can result in substantial cost savings.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedae566010ad14fe382cd_llm%20router.jpg)\\n\\n3\\\\. Multi-Agent Setup\\n---------------------\\n\\nAnother strategy is to set up multiple agents, each using a different model. The first agent attempts to complete the task using a cheaper model, and if it fails, the next agent is invoked. By using this multi-agent setup, you can achieve similar or even better success rates while significantly reducing costs.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaefa7461fca4f82df51_autogen.jpg)\\n\\n4\\\\. LLM Lingua\\n--------------\\n\\nLLM Lingua is a method introduced by Microsoft that focuses on optimizing the input and output of large language models. By removing unnecessary tokens and words from the input, you can significantly reduce the cost of running the model. This method is particularly effective for tasks such as summarization or answering specific questions based on a transcript.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)\\n\\n5\\\\. Optimize Agent Memory\\n-------------------------\\n\\nOptimizing agent memory is another way to reduce LLM costs. By carefully managing the amount of conversation history stored in memory, you can minimize the number of tokens required for each interaction. This can lead to significant cost savings, especially when dealing with long conversations.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedb03355b5d49f9ac7f8e_memory%20opt.jpg)\\n\\n6\\\\. Observability\\n-----------------\\n\\nHaving a deep understanding of the cost patterns in your LLM application is crucial for effective cost optimization. By using observability platforms like L Smith, you can monitor and log the cost for each large language model. This allows you to identify areas where costs can be optimized and make informed decisions to reduce overall expenses.\\n\\nBy implementing these strategies, you can reduce LLM costs by up to 78% or more. Remember, reducing costs while maintaining performance and user experience is a critical skill for AI startups. Stay proactive and continuously optimize your LLM usage to maximize efficiency and profitability.\\n\\n\\u200d\\n\\nGet free HubSpot AI For Marketers Course: [https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\n\\n🔗 Links\\n\\n*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n*   Join my AI email list: [https://crafters.ai/](https://crafters.ai/)\\n    \\n*   My discord: [https://discord.gg/eZXprSaCDE](https://discord.gg/eZXprSaCDE)\\n    \\n*   Inbox Agent: [https://www.youtube.com/watch?v=Jv\\\\_e6Rt4vWE&t=23s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=Jv_e6Rt4vWE&t=23s&ab_channel=AIJason)\\n    \\n*   Research Agent: [https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab_channel=AIJason)\\n    \\n*   James Brigg on Agent Memory: [https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)\\n    \\n*   Another video about details for LLM cost tracking: [https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab\\\\_channel=LearnfromOpenSourcewithElie](https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab_channel=LearnfromOpenSourcewithElie)\\n    \\n\\n\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### Q: How can I determine which model is the most cost-effective for my AI application?\\n\\nA: To determine the most cost-effective model for your AI application, you should consider the specific tasks and requirements of your application. Evaluate the performance and cost trade-offs of different models and choose the one that best fits your needs.\\n\\n### Q: Are there any open-source solutions available for large language model routing?\\n\\nA: While there are no specific open-source solutions for large language model routing, you can explore frameworks like Hugging Face\\'s Hugging GPT, which allows you to build your own routing logic using a large language model as a controller.\\n\\n### Q: How often should I monitor and optimize my LLM costs?\\n\\nA: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[The REAL cost of LLM (And How to reduce 78%+ of Cost)](https://www.youtube.com/watch?v=lHxl5SchjPA)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=lHxl5SchjPA \"Watch on YouTube\")', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nAI has come a long way in recent years, with large language models (LLMs) like GPT-4 impressing us with their ability to generate text. However, these models primarily rely on system one thinking, which is fast and intuitive but lacks the ability to break down complex problems into smaller steps and explore different options. This limitation has led researchers to focus on developing GPT-5 with enhanced reasoning abilities and reliability.\\n\\nThe Two Modes of Thinking\\n-------------------------\\n\\nIn his book \"Thinking, Fast and Slow,\" Daniel Kahneman introduces the concept of two modes of thinking: system one and system two. System one thinking is our fast, intuitive brain that quickly provides answers based on memorized information. On the other hand, system two thinking is slower but more rational, requiring us to take time, calculate, and analyze before arriving at an answer.\\n\\nSimilarly, large language models like GPT-4 primarily rely on system one thinking. They predict the best next words based on the sequence of words they have seen before, without truly understanding the complex problems they are trying to solve.\\n\\n![](https://assets-global.website-files.com/img/image-placeholder.svg)\\n\\n\\u200d\\n\\nThe Limitations of GPT-4\\n------------------------\\n\\nGPT-4, despite its impressive capabilities, lacks system two thinking. It cannot break down complex tasks into smaller steps or explore different options. It simply generates text based on patterns it has learned from training data. This limitation becomes evident when GPT-4 is faced with complex problems that require deeper analysis and reasoning.\\n\\nFor example, in a video by Veritasium, college students were asked seemingly simple questions like the time it takes for the Earth to go around the Sun. Many of them answered incorrectly because they relied on system one thinking, providing automatic intuitive answers without truly considering the question.\\n\\nLarge language models like GPT-4 face a similar challenge. They lack the ability to think critically and break down complex problems into smaller, manageable steps. This is where GPT-5 comes in.\\n\\nThe Promise of GPT-5\\n--------------------\\n\\nGPT-5 aims to enhance the reasoning abilities of large language models and introduce system two thinking. OpenAI\\'s Sam Altman mentioned in an interview with Bill Gates that the key milestones for GPT-5 will be around reasoning ability and reliability.\\n\\nCurrently, GPT-4 can reason in extremely limited ways and lacks reliability. It may provide correct answers, but it doesn\\'t always know which answer is the best. GPT-5 aims to improve this by increasing reliability and enhancing reasoning abilities.\\n\\nAltman also mentioned the possibility of GPT-5 being able to solve complex math equations by applying transformations an arbitrary number of times. This would require a more complex control logic for reasoning, going beyond what is currently possible with GPT-4.\\n\\nHowever, simply improving the model itself is not enough. There are ways to enforce system two thinking in large language models today, even with GPT-4.\\n\\nPromoting System 2 Thinking in Large Language Models\\n----------------------------------------------------\\n\\nThere are two common strategies to promote system two thinking in large language models: prompt engineering and communicative agents.\\n\\n### Prompt Engineering\\n\\nPrompt engineering is a simple and common method to guide large language models towards system two thinking. One approach is the \"chain of thought,\" where a sentence is inserted step by step before the model generates any text. This forces the model to break down the problem into smaller steps and think through each one.\\n\\nAnother approach is to provide a few short prompt examples instead of a step-by-step process. These examples guide the model towards thinking through different steps and considering multiple possibilities.\\n\\nWhile prompt engineering can be effective in promoting system two thinking, it has limitations. It often restricts the model to consider only one possibility and may not explore diverse options, similar to how humans approach creative problem-solving.\\n\\nTo address this limitation, more advanced prompting tactics like self-consistency with chain of thought (SCCOT) have been proposed. SCCOT involves running the chain of thought process multiple times and reviewing and voting on the most reasonable answers. This allows for some exploration of different options but requires more implementation effort.\\n\\nAnother advanced prompting tactic is the tree of sorts, which simulates a tree search to explore different options and paths. It keeps track of all the paths explored and allows for backtracking if the current path doesn\\'t lead to the desired outcome. However, implementing the tree of sorts is complex and requires significant implementation effort.\\n\\n### Communicative Agents\\n\\nCommunicative agents provide an elegant solution to promote system two thinking in large language models. These are multi-agent setups where users can define different agents and simulate conversations between them. The agents can reflect and spot flaws in each other\\'s perspectives and thinking processes.\\n\\nCommunicative agents have shown promise in enhancing system two thinking. They allow for dedicated agents to review and critique the model\\'s answers, identifying flaws and providing feedback. This collaborative approach mimics how humans solve complex problems by exploring multiple options and learning from each other.\\n\\nSetting up communicative agents can be done using various frameworks like ChatGPT, MetaGPT, Autogen, and Crew AI. These frameworks enable the creation of agent workflows and facilitate conversations between agents with different roles, such as problem solvers and reviewers.\\n\\nAutogen Studio, a no-code interface for Autogen, simplifies the setup of communicative agent workflows. It allows for easy collaboration and problem-solving between agents, making it accessible to a wider range of users.\\n\\nUnlocking System 2 Thinking with GPT-4 Today\\n--------------------------------------------\\n\\nWhile GPT-4 may not have native system two thinking capabilities, prompt engineering and communicative agents can be used to enforce system two thinking and solve complex tasks.\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides the model towards thinking through problems step by step and considering multiple possibilities. However, prompt engineering may limit exploration and diversity of solutions.\\n\\nCommunicative agents, on the other hand, provide a collaborative approach to problem-solving. By simulating conversations between agents, users can leverage the strengths of system one and system two thinking. Reviewers can spot flaws in the model\\'s answers, while problem solvers can iterate and improve their solutions based on feedback.\\n\\nFrameworks like Autogen Studio make it easy to set up communicative agent workflows, allowing for seamless collaboration and problem-solving.\\n\\nThe Future of GPT-5 and System 2 Thinking\\n-----------------------------------------\\n\\nGPT-5 holds the promise of unlocking system two thinking in large language models. With enhanced reasoning abilities and reliability, GPT-5 aims to bridge the gap between system one and system two thinking, enabling models to solve complex problems more effectively.\\n\\nResearchers are actively working on developing GPT-5 with improved reasoning abilities. The focus is on enabling large language models to break down complex tasks, explore different options, and make more accurate and informed decisions.\\n\\nAs we look forward to the advancements in GPT-5, it\\'s important to continue exploring and implementing strategies like prompt engineering and communicative agents to drive system two thinking in large language models today.\\n\\n\\u200d\\n\\nFollow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n\\n\\u200d\\n\\nFAQs\\n----\\n\\n### 1\\\\. Can GPT-4 solve complex problems?\\n\\nGPT-4 can generate text and provide answers, but it primarily relies on system one thinking. It lacks the ability to break down complex problems into smaller steps and explore different options.\\n\\n### 2\\\\. How can prompt engineering promote system two thinking?\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides large language models towards thinking through problems step by step and considering multiple possibilities.\\n\\n### 3\\\\. What are communicative agents?\\n\\nCommunicative agents are multi-agent setups where users can define different agents and simulate conversations between them. This allows for collaborative problem-solving and promotes system two thinking.\\n\\n### 4\\\\. How can communicative agents be set up?\\n\\nFrameworks like Autogen Studio provide a no-code interface for setting up communicative agent workflows. Users can define agents, assign roles, and simulate conversations to solve complex problems.\\n\\n### 5\\\\. What is the future of GPT-5?\\n\\nGPT-5 aims to enhance reasoning abilities and bridge the gap between system one and system two thinking. It holds the promise of enabling large language models to solve complex problems more effectively.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[GPT5 unlocks LLM System 2 Thinking?](https://www.youtube.com/watch?v=sD0X-lWPdxg)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=sD0X-lWPdxg \"Watch on YouTube\")', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to Build Agent workforce Tutorial- AI agent manages community 24/7\\n======================================================================\\n\\nAI agent manages community 24/7 - Build Agent workforce\\n=======================================================\\n\\nAre you overwhelmed with the amount of emails, messages, and tasks you need to handle every day? Do you wish you had a personal assistant to help you manage your workload? Well, thanks to advancements in artificial intelligence (AI), you can now build your own AI employees to take care of these tasks for you. In this series of videos, I will show you how to create AI agents that can act as your digital workforce, handling various roles and tasks.\\n\\nWhy Build AI Employees?\\n-----------------------\\n\\nAs a content creator with a growing YouTube channel, I found myself struggling to keep up with the influx of emails, messages from various platforms, and the need to stay on top of new purchases and video editing work. I considered outsourcing some of the work or hiring someone to help me, but then I had an idea. Instead of relying on others, why not build AI employees to handle these tasks? This would not only be an interesting experiment but also a way to push the boundaries of AI technology.\\n\\nBuilding AI employees is similar to building self-driving cars. While many researchers are focused on developing fully autonomous AI agents (level 5), there are plenty of opportunities to create AI agents that require some human guidance (level 3 or level 2). These AI agents can still deliver excellent results and are more feasible to build at this stage. My goal is not to create fully autonomous agents but to build AI employees that can get the work done with some guidance.\\n\\nSo, the first AI employee I want to build is a community moderator for my Discord community. As a content creator based in Australia, I have community members from different parts of the world with wildly different time zones. It\\'s challenging for me to respond to messages in a timely manner. Ideally, I would love to have a community moderator who can proactively engage with people 24/7. This AI moderator should have deep domain knowledge in AI and software development, stay updated on AI trends, and engage with the community proactively.\\n\\nChallenges to Overcome\\n----------------------\\n\\nBuilding an AI community moderator comes with its own set of challenges. Firstly, how should the AI behave in a group chat setup? Most AI agents are designed for one-on-one conversations, but in a Discord community, there can be multiple people talking at the same time. Secondly, the AI moderator should have a long-term memory of each community member to make conversations feel more genuine and to connect community members with similar interests. Lastly, the AI moderator should be able to retrieve knowledge from various sources, such as my YouTube channel, GitHub repository, and the internet, to provide up-to-date information.\\n\\nTo overcome these challenges, I found an open-source project called Discord AI chatbot on GitHub. This project integrates OpenAI into a Discord bot, taking care of the network setup and providing design patterns for AI behavior in a group chat setup. It allows me to define trigger words that activate the AI moderator and retrieve the relevant conversation history. While this project has some limitations, such as limited conversation history and the inability to perform complex tasks, it provides a flexible foundation for customization.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee0a5f2e303fd2cefe0b3_Copy%20of%20Your%20paragraph%20text.jpg)\\n\\nGetting Started: Installing Discord AI Chatbot\\n----------------------------------------------\\n\\nTo get started, you need to install the Discord AI chatbot. Once the AI chatbot is up and running, you can customize its behavior by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.\\n\\nEnhancing the AI Moderator: Building an AI Agent\\n------------------------------------------------\\n\\nWhile the AI chatbot provides a good starting point, I wanted to create an AI moderator with more advanced capabilities. I wanted the AI moderator to have a long-term memory, retrieve knowledge from various sources, and schedule tasks. To achieve this, I used the Launching Agent framework and integrated it into the AI chatbot.\\n\\nFirstly, I created a knowledge retrieval function to extract information from my YouTube channel, GitHub repository, and the internet. I used a platform called Random CI to turn my website into a knowledge base and used the Launching Agent\\'s large language model to perform knowledge retrieval. I also implemented functions for online research, such as Google search and website scripting, to gather information from the internet.\\n\\nNext, I created a research agent that combines the knowledge retrieval function and online research tools. This research agent can search my internal knowledge base first and then perform online research if necessary. It can provide relevant information and links to answer questions from the community.\\n\\nTo integrate the research agent into the AI chatbot, I modified the AI utility file. I updated the message handling function to pass the user\\'s message to the research agent and added a mapping of agents for each user. This allows each user to have a unique chat history and memory. I also added a scheduler feature to trigger the research agent to generate GitHub reports every two days.\\n\\nGenerating GitHub Reports: Sharing AI Trends and News\\n-----------------------------------------------------\\n\\nIn addition to moderating the community, I wanted the AI agent to share the latest AI trends and news from various sources, such as Twitter, GitHub, and newsletters. For the initial MVP, I decided to generate GitHub reports every two days. The AI agent would check the GitHub trending page, filter out projects related to generative AI, and write a report to keep the community updated.\\n\\nTo achieve this, I used a platform called BrowseAI to extract structured data from the GitHub trending page. BrowseAI allows me to define the elements I want to extract, such as the name of the repo, the link, and the number of stars. I integrated BrowseAI into the AI agent to script the GitHub trending page and retrieve the relevant information.\\n\\nI also implemented a scheduler feature in the AI chatbot to trigger the generation of GitHub reports every two days. This feature allows the AI agent to automatically post the reports in the Discord channels that have enabled the schedule task.\\n\\nDeployment and Continuous Operation\\n-----------------------------------\\n\\nTo ensure the AI moderator operates 24/7, you can deploy the AI chatbot on a cloud service. One option is to use Render, a platform that allows you to deploy web services. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nHowever, keep in mind that free versions of cloud services may have limitations, such as inactivity timeouts. To prevent the AI moderator from going inactive, you can use uptime monitoring services like Uptime Robot to ping your API endpoint at regular intervals.\\n\\nWith your AI moderator deployed and continuously operational, you can enjoy the benefits of having a 24/7 community manager. The AI moderator will handle messages, answer questions, and share the latest AI trends and news, allowing you to focus on other aspects of your content creation.\\n\\nConclusion\\n----------\\n\\nBuilding AI employees, such as a community moderator, can greatly enhance your productivity and efficiency. With the right tools and frameworks, you can create AI agents that handle various tasks and roles, providing a seamless experience for your community members. By combining knowledge retrieval, online research, and scheduling capabilities, you can build an AI moderator that stays engaged with your community 24/7.\\n\\n🔗 Links\\n\\n*   Github repo: [https://github.com/JayZeeDesign/Discord-AI-Chatbot](https://github.com/JayZeeDesign/Discord-AI-Chatbot)\\n    \\n*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n\\n\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### 1\\\\. Can I customize the behavior of the AI moderator?\\n\\nYes, you can customize the behavior of the AI moderator by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.\\n\\n### 2\\\\. Can the AI moderator handle multiple conversations in a group chat setup?\\n\\nYes, the AI moderator is designed to handle group chat setups. It can understand the context of the conversation and respond accordingly, even when multiple people are talking at the same time.\\n\\n### 3\\\\. How does the AI moderator retrieve knowledge from different sources?\\n\\nThe AI moderator can retrieve knowledge from various sources, such as your YouTube channel, GitHub repository, and the internet. It uses a combination of knowledge retrieval techniques and online research tools to gather relevant information.\\n\\n### 4\\\\. Can the AI moderator generate reports and share AI trends?\\n\\nYes, the AI moderator can generate reports and share the latest AI trends. It can retrieve information from GitHub, filter out projects related to generative AI, and write reports to keep the community updated.\\n\\n### 5\\\\. How can I deploy the AI moderator on a cloud service?\\n\\nTo deploy the AI moderator on a cloud service, you can use platforms like Render. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[AI agent manages community 24/7 - Build Agent workforce ep#1](https://www.youtube.com/watch?v=yhBiVrigWNI)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=yhBiVrigWNI \"Watch on YouTube\")', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'ogLocaleAlternate': [], 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'})]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_split = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n",
      "{'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}\n"
     ]
    }
   ],
   "source": [
    "for doc in doc_split:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k:v for k,v in doc.metadata.items() if isinstance(v, (str, int, float, bool)) and len(v) > 0}\n",
    "        print(clean_metadata)\n",
    "        filtered_docs.append(Document(page_content=doc.page_content,metadata=clean_metadata))\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nHow to reduce 78%+ of LLM Cost\\n==============================\\n\\nAre you building AI agents or using chatGPT? If so, you may be facing the challenge of high costs associated with large language models (LLM). In this article, we will explore effective strategies to reduce LLM costs by up to 78%. Let's dive in!\\n\\n\\u200d\\n\\n1\\\\. Change Model\\n----------------\", metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='One effective way to reduce LLM costs is to change the model you are using. Different models have different costs associated with them. For example, GPT-4 is the most powerful but also the most expensive model, while Mistro 7B is significantly cheaper. By using a smaller model for specific tasks and reserving the more expensive model for complex questions, you can achieve significant cost savings.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg)\\n\\n2\\\\. Large Language Model Router\\n-------------------------------\\n\\nThe concept of a large language model router involves using a cascade of models to handle different types of questions. Cheaper models are used first, and if they are unable to provide a satisfactory answer, the question is passed on to a more expensive model. This approach leverages the significant cost difference between models and can result in substantial cost savings.', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedae566010ad14fe382cd_llm%20router.jpg)\\n\\n3\\\\. Multi-Agent Setup\\n---------------------\\n\\nAnother strategy is to set up multiple agents, each using a different model. The first agent attempts to complete the task using a cheaper model, and if it fails, the next agent is invoked. By using this multi-agent setup, you can achieve similar or even better success rates while significantly reducing costs.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaefa7461fca4f82df51_autogen.jpg)\\n\\n4\\\\. LLM Lingua\\n--------------\\n\\nLLM Lingua is a method introduced by Microsoft that focuses on optimizing the input and output of large language models. By removing unnecessary tokens and words from the input, you can significantly reduce the cost of running the model. This method is particularly effective for tasks such as summarization or answering specific questions based on a transcript.', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)\\n\\n5\\\\. Optimize Agent Memory\\n-------------------------\\n\\nOptimizing agent memory is another way to reduce LLM costs. By carefully managing the amount of conversation history stored in memory, you can minimize the number of tokens required for each interaction. This can lead to significant cost savings, especially when dealing with long conversations.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedb03355b5d49f9ac7f8e_memory%20opt.jpg)\\n\\n6\\\\. Observability\\n-----------------\\n\\nHaving a deep understanding of the cost patterns in your LLM application is crucial for effective cost optimization. By using observability platforms like L Smith, you can monitor and log the cost for each large language model. This allows you to identify areas where costs can be optimized and make informed decisions to reduce overall expenses.', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='By implementing these strategies, you can reduce LLM costs by up to 78% or more. Remember, reducing costs while maintaining performance and user experience is a critical skill for AI startups. Stay proactive and continuously optimize your LLM usage to maximize efficiency and profitability.\\n\\n\\u200d\\n\\nGet free HubSpot AI For Marketers Course: [https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\n\\n🔗 Links', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n*   Join my AI email list: [https://crafters.ai/](https://crafters.ai/)\\n    \\n*   My discord: [https://discord.gg/eZXprSaCDE](https://discord.gg/eZXprSaCDE)\\n    \\n*   Inbox Agent: [https://www.youtube.com/watch?v=Jv\\\\_e6Rt4vWE&t=23s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=Jv_e6Rt4vWE&t=23s&ab_channel=AIJason)\\n    \\n*   Research Agent: [https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab\\\\_channel=AIJason](https://www.youtube.com/watch?v=ogQUlS7CkYA&t=299s&ab_channel=AIJason)\\n    \\n*   James Brigg on Agent Memory: [https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='*   Another video about details for LLM cost tracking: [https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab\\\\_channel=LearnfromOpenSourcewithElie](https://www.youtube.com/watch?v=Alb2kjUzpZ8&ab_channel=LearnfromOpenSourcewithElie)', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content=\"\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### Q: How can I determine which model is the most cost-effective for my AI application?\\n\\nA: To determine the most cost-effective model for your AI application, you should consider the specific tasks and requirements of your application. Evaluate the performance and cost trade-offs of different models and choose the one that best fits your needs.\\n\\n### Q: Are there any open-source solutions available for large language model routing?\\n\\nA: While there are no specific open-source solutions for large language model routing, you can explore frameworks like Hugging Face's Hugging GPT, which allows you to build your own routing logic using a large language model as a controller.\\n\\n### Q: How often should I monitor and optimize my LLM costs?\", metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[The REAL cost of LLM (And How to reduce 78%+ of Cost)](https://www.youtube.com/watch?v=lHxl5SchjPA)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=lHxl5SchjPA \"Watch on YouTube\")', metadata={'title': 'How to reduce 78%+ of LLM Cost', 'description': 'AI Agent, Real cost of LLM', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nGPT5 unlocks LLM System 2 Thinking?\\n===================================\\n\\nAI has come a long way in recent years, with large language models (LLMs) like GPT-4 impressing us with their ability to generate text. However, these models primarily rely on system one thinking, which is fast and intuitive but lacks the ability to break down complex problems into smaller steps and explore different options. This limitation has led researchers to focus on developing GPT-5 with enhanced reasoning abilities and reliability.', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='The Two Modes of Thinking\\n-------------------------\\n\\nIn his book \"Thinking, Fast and Slow,\" Daniel Kahneman introduces the concept of two modes of thinking: system one and system two. System one thinking is our fast, intuitive brain that quickly provides answers based on memorized information. On the other hand, system two thinking is slower but more rational, requiring us to take time, calculate, and analyze before arriving at an answer.\\n\\nSimilarly, large language models like GPT-4 primarily rely on system one thinking. They predict the best next words based on the sequence of words they have seen before, without truly understanding the complex problems they are trying to solve.\\n\\n![](https://assets-global.website-files.com/img/image-placeholder.svg)\\n\\n\\u200d\\n\\nThe Limitations of GPT-4\\n------------------------', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='GPT-4, despite its impressive capabilities, lacks system two thinking. It cannot break down complex tasks into smaller steps or explore different options. It simply generates text based on patterns it has learned from training data. This limitation becomes evident when GPT-4 is faced with complex problems that require deeper analysis and reasoning.\\n\\nFor example, in a video by Veritasium, college students were asked seemingly simple questions like the time it takes for the Earth to go around the Sun. Many of them answered incorrectly because they relied on system one thinking, providing automatic intuitive answers without truly considering the question.\\n\\nLarge language models like GPT-4 face a similar challenge. They lack the ability to think critically and break down complex problems into smaller, manageable steps. This is where GPT-5 comes in.\\n\\nThe Promise of GPT-5\\n--------------------', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content=\"GPT-5 aims to enhance the reasoning abilities of large language models and introduce system two thinking. OpenAI's Sam Altman mentioned in an interview with Bill Gates that the key milestones for GPT-5 will be around reasoning ability and reliability.\\n\\nCurrently, GPT-4 can reason in extremely limited ways and lacks reliability. It may provide correct answers, but it doesn't always know which answer is the best. GPT-5 aims to improve this by increasing reliability and enhancing reasoning abilities.\\n\\nAltman also mentioned the possibility of GPT-5 being able to solve complex math equations by applying transformations an arbitrary number of times. This would require a more complex control logic for reasoning, going beyond what is currently possible with GPT-4.\\n\\nHowever, simply improving the model itself is not enough. There are ways to enforce system two thinking in large language models today, even with GPT-4.\", metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='Promoting System 2 Thinking in Large Language Models\\n----------------------------------------------------\\n\\nThere are two common strategies to promote system two thinking in large language models: prompt engineering and communicative agents.\\n\\n### Prompt Engineering\\n\\nPrompt engineering is a simple and common method to guide large language models towards system two thinking. One approach is the \"chain of thought,\" where a sentence is inserted step by step before the model generates any text. This forces the model to break down the problem into smaller steps and think through each one.\\n\\nAnother approach is to provide a few short prompt examples instead of a step-by-step process. These examples guide the model towards thinking through different steps and considering multiple possibilities.', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content=\"While prompt engineering can be effective in promoting system two thinking, it has limitations. It often restricts the model to consider only one possibility and may not explore diverse options, similar to how humans approach creative problem-solving.\\n\\nTo address this limitation, more advanced prompting tactics like self-consistency with chain of thought (SCCOT) have been proposed. SCCOT involves running the chain of thought process multiple times and reviewing and voting on the most reasonable answers. This allows for some exploration of different options but requires more implementation effort.\\n\\nAnother advanced prompting tactic is the tree of sorts, which simulates a tree search to explore different options and paths. It keeps track of all the paths explored and allows for backtracking if the current path doesn't lead to the desired outcome. However, implementing the tree of sorts is complex and requires significant implementation effort.\\n\\n### Communicative Agents\", metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content=\"Communicative agents provide an elegant solution to promote system two thinking in large language models. These are multi-agent setups where users can define different agents and simulate conversations between them. The agents can reflect and spot flaws in each other's perspectives and thinking processes.\\n\\nCommunicative agents have shown promise in enhancing system two thinking. They allow for dedicated agents to review and critique the model's answers, identifying flaws and providing feedback. This collaborative approach mimics how humans solve complex problems by exploring multiple options and learning from each other.\\n\\nSetting up communicative agents can be done using various frameworks like ChatGPT, MetaGPT, Autogen, and Crew AI. These frameworks enable the creation of agent workflows and facilitate conversations between agents with different roles, such as problem solvers and reviewers.\", metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='Autogen Studio, a no-code interface for Autogen, simplifies the setup of communicative agent workflows. It allows for easy collaboration and problem-solving between agents, making it accessible to a wider range of users.\\n\\nUnlocking System 2 Thinking with GPT-4 Today\\n--------------------------------------------\\n\\nWhile GPT-4 may not have native system two thinking capabilities, prompt engineering and communicative agents can be used to enforce system two thinking and solve complex tasks.\\n\\nPrompt engineering, such as the chain of thought or self-consistency with chain of thought, guides the model towards thinking through problems step by step and considering multiple possibilities. However, prompt engineering may limit exploration and diversity of solutions.', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content=\"Communicative agents, on the other hand, provide a collaborative approach to problem-solving. By simulating conversations between agents, users can leverage the strengths of system one and system two thinking. Reviewers can spot flaws in the model's answers, while problem solvers can iterate and improve their solutions based on feedback.\\n\\nFrameworks like Autogen Studio make it easy to set up communicative agent workflows, allowing for seamless collaboration and problem-solving.\\n\\nThe Future of GPT-5 and System 2 Thinking\\n-----------------------------------------\\n\\nGPT-5 holds the promise of unlocking system two thinking in large language models. With enhanced reasoning abilities and reliability, GPT-5 aims to bridge the gap between system one and system two thinking, enabling models to solve complex problems more effectively.\", metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content=\"Researchers are actively working on developing GPT-5 with improved reasoning abilities. The focus is on enabling large language models to break down complex tasks, explore different options, and make more accurate and informed decisions.\\n\\nAs we look forward to the advancements in GPT-5, it's important to continue exploring and implementing strategies like prompt engineering and communicative agents to drive system two thinking in large language models today.\\n\\n\\u200d\\n\\nFollow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n\\n\\u200d\\n\\nFAQs\\n----\\n\\n### 1\\\\. Can GPT-4 solve complex problems?\\n\\nGPT-4 can generate text and provide answers, but it primarily relies on system one thinking. It lacks the ability to break down complex problems into smaller steps and explore different options.\\n\\n### 2\\\\. How can prompt engineering promote system two thinking?\", metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='Prompt engineering, such as the chain of thought or self-consistency with chain of thought, guides large language models towards thinking through problems step by step and considering multiple possibilities.\\n\\n### 3\\\\. What are communicative agents?\\n\\nCommunicative agents are multi-agent setups where users can define different agents and simulate conversations between them. This allows for collaborative problem-solving and promotes system two thinking.\\n\\n### 4\\\\. How can communicative agents be set up?\\n\\nFrameworks like Autogen Studio provide a no-code interface for setting up communicative agent workflows. Users can define agents, assign roles, and simulate conversations to solve complex problems.\\n\\n### 5\\\\. What is the future of GPT-5?\\n\\nGPT-5 aims to enhance reasoning abilities and bridge the gap between system one and system two thinking. It holds the promise of enabling large language models to solve complex problems more effectively.\\n\\nRelated articles\\n----------------', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to Build Agent workforce Tutorial- AI agent manages community 24/7\\\\\\n\\\\\\n![How to Build Agent workforce Tutorial- AI agent manages community 24/7](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg)](/learning-ai/ai-agent-tutorial-2)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[GPT5 unlocks LLM System 2 Thinking?](https://www.youtube.com/watch?v=sD0X-lWPdxg)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=sD0X-lWPdxg \"Watch on YouTube\")', metadata={'title': 'GPT5 unlocks LLM System 2 Thinking?', 'description': 'AI Agent,', 'ogTitle': 'GPT5 unlocks LLM System 2 Thinking?', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/gpt5-llm'}),\n",
       " Document(page_content='[AI JASON](/)\\n\\nCategories\\n\\n\\ue80f\\n\\n[AI Agent](/ai/ai-agent)\\n\\n[AI Automation Tutorials](/ai/ai-automation-tutorials)\\n\\n[Langchain Tutorials](/ai/langchain-tutorial)\\n\\n[Auto GPT Tutorials](/ai/auto-gpt-tutorial)\\n\\n[Contact](/contact)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\n[AI Agent](/ai/ai-agent)\\n\\nHow to Build Agent workforce Tutorial- AI agent manages community 24/7\\n======================================================================\\n\\nAI agent manages community 24/7 - Build Agent workforce\\n=======================================================', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='Are you overwhelmed with the amount of emails, messages, and tasks you need to handle every day? Do you wish you had a personal assistant to help you manage your workload? Well, thanks to advancements in artificial intelligence (AI), you can now build your own AI employees to take care of these tasks for you. In this series of videos, I will show you how to create AI agents that can act as your digital workforce, handling various roles and tasks.\\n\\nWhy Build AI Employees?\\n-----------------------', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='As a content creator with a growing YouTube channel, I found myself struggling to keep up with the influx of emails, messages from various platforms, and the need to stay on top of new purchases and video editing work. I considered outsourcing some of the work or hiring someone to help me, but then I had an idea. Instead of relying on others, why not build AI employees to handle these tasks? This would not only be an interesting experiment but also a way to push the boundaries of AI technology.\\n\\nBuilding AI employees is similar to building self-driving cars. While many researchers are focused on developing fully autonomous AI agents (level 5), there are plenty of opportunities to create AI agents that require some human guidance (level 3 or level 2). These AI agents can still deliver excellent results and are more feasible to build at this stage. My goal is not to create fully autonomous agents but to build AI employees that can get the work done with some guidance.', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content=\"So, the first AI employee I want to build is a community moderator for my Discord community. As a content creator based in Australia, I have community members from different parts of the world with wildly different time zones. It's challenging for me to respond to messages in a timely manner. Ideally, I would love to have a community moderator who can proactively engage with people 24/7. This AI moderator should have deep domain knowledge in AI and software development, stay updated on AI trends, and engage with the community proactively.\\n\\nChallenges to Overcome\\n----------------------\", metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='Building an AI community moderator comes with its own set of challenges. Firstly, how should the AI behave in a group chat setup? Most AI agents are designed for one-on-one conversations, but in a Discord community, there can be multiple people talking at the same time. Secondly, the AI moderator should have a long-term memory of each community member to make conversations feel more genuine and to connect community members with similar interests. Lastly, the AI moderator should be able to retrieve knowledge from various sources, such as my YouTube channel, GitHub repository, and the internet, to provide up-to-date information.', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='To overcome these challenges, I found an open-source project called Discord AI chatbot on GitHub. This project integrates OpenAI into a Discord bot, taking care of the network setup and providing design patterns for AI behavior in a group chat setup. It allows me to define trigger words that activate the AI moderator and retrieve the relevant conversation history. While this project has some limitations, such as limited conversation history and the inability to perform complex tasks, it provides a flexible foundation for customization.\\n\\n![](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee0a5f2e303fd2cefe0b3_Copy%20of%20Your%20paragraph%20text.jpg)\\n\\nGetting Started: Installing Discord AI Chatbot\\n----------------------------------------------', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='To get started, you need to install the Discord AI chatbot. Once the AI chatbot is up and running, you can customize its behavior by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.\\n\\nEnhancing the AI Moderator: Building an AI Agent\\n------------------------------------------------\\n\\nWhile the AI chatbot provides a good starting point, I wanted to create an AI moderator with more advanced capabilities. I wanted the AI moderator to have a long-term memory, retrieve knowledge from various sources, and schedule tasks. To achieve this, I used the Launching Agent framework and integrated it into the AI chatbot.', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content=\"Firstly, I created a knowledge retrieval function to extract information from my YouTube channel, GitHub repository, and the internet. I used a platform called Random CI to turn my website into a knowledge base and used the Launching Agent's large language model to perform knowledge retrieval. I also implemented functions for online research, such as Google search and website scripting, to gather information from the internet.\\n\\nNext, I created a research agent that combines the knowledge retrieval function and online research tools. This research agent can search my internal knowledge base first and then perform online research if necessary. It can provide relevant information and links to answer questions from the community.\", metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content=\"To integrate the research agent into the AI chatbot, I modified the AI utility file. I updated the message handling function to pass the user's message to the research agent and added a mapping of agents for each user. This allows each user to have a unique chat history and memory. I also added a scheduler feature to trigger the research agent to generate GitHub reports every two days.\\n\\nGenerating GitHub Reports: Sharing AI Trends and News\\n-----------------------------------------------------\\n\\nIn addition to moderating the community, I wanted the AI agent to share the latest AI trends and news from various sources, such as Twitter, GitHub, and newsletters. For the initial MVP, I decided to generate GitHub reports every two days. The AI agent would check the GitHub trending page, filter out projects related to generative AI, and write a report to keep the community updated.\", metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='To achieve this, I used a platform called BrowseAI to extract structured data from the GitHub trending page. BrowseAI allows me to define the elements I want to extract, such as the name of the repo, the link, and the number of stars. I integrated BrowseAI into the AI agent to script the GitHub trending page and retrieve the relevant information.\\n\\nI also implemented a scheduler feature in the AI chatbot to trigger the generation of GitHub reports every two days. This feature allows the AI agent to automatically post the reports in the Discord channels that have enabled the schedule task.\\n\\nDeployment and Continuous Operation\\n-----------------------------------', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='To ensure the AI moderator operates 24/7, you can deploy the AI chatbot on a cloud service. One option is to use Render, a platform that allows you to deploy web services. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nHowever, keep in mind that free versions of cloud services may have limitations, such as inactivity timeouts. To prevent the AI moderator from going inactive, you can use uptime monitoring services like Uptime Robot to ping your API endpoint at regular intervals.\\n\\nWith your AI moderator deployed and continuously operational, you can enjoy the benefits of having a 24/7 community manager. The AI moderator will handle messages, answer questions, and share the latest AI trends and news, allowing you to focus on other aspects of your content creation.\\n\\nConclusion\\n----------', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='Building AI employees, such as a community moderator, can greatly enhance your productivity and efficiency. With the right tools and frameworks, you can create AI agents that handle various tasks and roles, providing a seamless experience for your community members. By combining knowledge retrieval, online research, and scheduling capabilities, you can build an AI moderator that stays engaged with your community 24/7.\\n\\n🔗 Links\\n\\n*   Github repo: [https://github.com/JayZeeDesign/Discord-AI-Chatbot](https://github.com/JayZeeDesign/Discord-AI-Chatbot)\\n    \\n*   Follow me on twitter: [https://twitter.com/jasonzhou1993](https://twitter.com/jasonzhou1993)\\n    \\n\\n\\u200d\\n\\nFrequently Asked Questions\\n--------------------------\\n\\n### 1\\\\. Can I customize the behavior of the AI moderator?\\n\\nYes, you can customize the behavior of the AI moderator by modifying the config.yaml file. You can change the name of the bot, define trigger words, and add predefined personalities to give the bot a specific character.', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='### 2\\\\. Can the AI moderator handle multiple conversations in a group chat setup?\\n\\nYes, the AI moderator is designed to handle group chat setups. It can understand the context of the conversation and respond accordingly, even when multiple people are talking at the same time.\\n\\n### 3\\\\. How does the AI moderator retrieve knowledge from different sources?\\n\\nThe AI moderator can retrieve knowledge from various sources, such as your YouTube channel, GitHub repository, and the internet. It uses a combination of knowledge retrieval techniques and online research tools to gather relevant information.\\n\\n### 4\\\\. Can the AI moderator generate reports and share AI trends?\\n\\nYes, the AI moderator can generate reports and share the latest AI trends. It can retrieve information from GitHub, filter out projects related to generative AI, and write reports to keep the community updated.\\n\\n### 5\\\\. How can I deploy the AI moderator on a cloud service?', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='To deploy the AI moderator on a cloud service, you can use platforms like Render. Simply upload your project to GitHub, create a new web service on Render, and configure the necessary environment variables. Render will handle the deployment and ensure your AI moderator stays active.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)\\n\\n[AI Agent\\\\\\n\\\\\\n### How to reduce 78%+ of LLM Cost\\\\\\n\\\\\\n![How to reduce 78%+ of LLM Cost](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg)](/learning-ai/how-to-reduce-llm-cost)\\n\\n[AI Agent\\\\\\n\\\\\\n### GPT5 unlocks LLM System 2 Thinking?\\\\\\n\\\\\\n![GPT5 unlocks LLM System 2 Thinking?\\\\\\n](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedd7f060ee7802639f834_tiktok%20(2).jpg)](/learning-ai/gpt5-llm)', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content=\"[AI Agent\\\\\\n\\\\\\n### Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial\\\\\\n\\\\\\n![Research agent 3.0 - Build a group of AI researchers - Step by Step Tutorial](https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedeabdecca18351fc45d0_tiktok%20(3).jpg)](/learning-ai/how-to-build-ai-agent-tutorial-3)\\n\\n### Need help building your AI apps?\\n\\nI can help you on building AI apps or give trainings\\n\\n[Tell me about your AI app ideas](mailto:jason.zhou.design@gmail.com)\\n\\n[\\ue82e](https://www.youtube.com/@AIJason-oc8wb)\\n[\\ue829](https://twitter.com/jasonzhou1993)\\n[\\ue819](https://www.linkedin.com/in/jasonzhoudesign/)\\n\\nAI Jason\\n\\n100K subscribers\\n\\n[AI agent manages community 24/7 - Build Agent workforce ep#1](https://www.youtube.com/watch?v=yhBiVrigWNI)\\n\\nAI Jason\\n\\nSearch\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn't begin shortly, try restarting your device.\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\nWatch later\", metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'}),\n",
       " Document(page_content='Share\\n\\nCopy link\\n\\nWatch on\\n\\n0:00\\n\\n/ •Live\\n\\n•\\n\\n[](https://www.youtube.com/watch?v=yhBiVrigWNI \"Watch on YouTube\")', metadata={'title': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'description': 'AI Agent,', 'ogTitle': 'How to Build Agent workforce Tutorial- AI agent manages community 24/7', 'ogDescription': 'AI Agent,', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bee17260bcfa83d917651f_tiktok%20(6).jpg', 'sourceURL': 'https://www.ai-jason.com/learning-ai/ai-agent-tutorial-2'})]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=filtered_docs, collection_name=\"rag-chromadb\", embedding=GPT4AllEmbeddings(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever to retrieve from the vectorstore\n",
    "retriver = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever grader will grade whether documents retrieved from the retriever are relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3\n"
     ]
    }
   ],
   "source": [
    "print(local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model_name=local_llm, format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['document', 'question'], template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\\n     <|eot_id|><|start_header_id|>user<|end_header_id|>\\n    Here is the retrieved document: \\n\\n {document} \\n\\n\\n    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n    \")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(temperature=0.0, format='json')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_grader= prompt| llm| JsonOutputParser()\n",
    "question = \"How to reduce llm cost\"\n",
    "docs = retriver.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='By implementing these strategies, you can reduce LLM costs by up to 78% or more. Remember, reducing costs while maintaining performance and user experience is a critical skill for AI startups. Stay proactive and continuously optimize your LLM usage to maximize efficiency and profitability.\\n\\n\\u200d\\n\\nGet free HubSpot AI For Marketers Course: [https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\n\\n🔗 Links', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_txt = docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_grader= prompt| llm| JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['document', 'question'], template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \\n    of a retrieved document to a user question. If the document contains keywords related to the user question, \\n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\\n    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\\n     <|eot_id|><|start_header_id|>user<|end_header_id|>\\n    Here is the retrieved document: \\n\\n {document} \\n\\n\\n    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n    \")\n",
       "| ChatOllama(temperature=0.0, format='json')\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(temperature=0.0, format='json')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How to reduce llm cost',\n",
       " 'A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question,doc_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (base) genai@genais-MacBook-Pro ~ % ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mretrival_grader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mdoc_txt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:259\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    237\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    267\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    268\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:190\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    183\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    188\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    189\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:162\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    161\u001b[0m     }\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_community/llms/ollama.py:244\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "source": [
    "print(retrival_grader.invoke({\"question\":question, \"document\":doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/crewai_test/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a fascinating topic!\n",
      "\n",
      "The history of Artificial Intelligence (AI) dates back to ancient civilizations, but I'll focus on the modern era. Here's an overview:\n",
      "\n",
      "**Early Beginnings**\n",
      "\n",
      "* 1950s: The Dartmouth Summer Research Project on Artificial Intelligence (DSRPAI) was established, marking the birth of AI as a distinct field. John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon were among the key figures.\n",
      "* 1951: Alan Turing published \"Computing Machinery and Intelligence,\" a paper that proposed the Turing Test, a measure of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\n",
      "\n",
      "**Golden Age**\n",
      "\n",
      "* 1960s: The term \"Artificial Intelligence\" was coined by John McCarthy. This era saw significant advancements in AI research:\n",
      "\t+ Rule-based systems (e.g., Mycin, a medical diagnosis program)\n",
      "\t+ Logic programming (e.g., Prolog)\n",
      "\t+ Machine learning (e.g., decision trees)\n",
      "\n",
      "**AI Winter**\n",
      "\n",
      "* 1970s-1980s: Despite the initial excitement and progress, AI research faced significant challenges and funding cuts. This period became known as the \"AI winter.\" Reasons included:\n",
      "\t+ Limited understanding of human intelligence\n",
      "\t+ Lack of computational power and data storage\n",
      "\t+ High expectations vs. slow progress\n",
      "\n",
      "**Resurgence**\n",
      "\n",
      "* 1980s-1990s: AI research experienced a resurgence, driven by advances in computing power, memory, and data storage. This led to the development of:\n",
      "\t+ Expert systems (e.g., MYCIN, R1)\n",
      "\t+ Knowledge representation languages (e.g., frames, semantic networks)\n",
      "\n",
      "**Modern Era**\n",
      "\n",
      "* 2000s: AI research accelerated with the rise of:\n",
      "\t+ Machine learning (ML) and deep learning (DL)\n",
      "\t+ Natural Language Processing (NLP) and computer vision\n",
      "\t+ Big data and the Internet of Things (IoT)\n",
      "\t+ Open-source frameworks like TensorFlow, PyTorch, and Keras\n",
      "\n",
      "**Current State**\n",
      "\n",
      "* AI is now a rapidly growing field, with applications in:\n",
      "\t+ Healthcare (e.g., medical diagnosis, personalized medicine)\n",
      "\t+ Finance (e.g., risk analysis, portfolio optimization)\n",
      "\t+ Transportation (e.g., autonomous vehicles, traffic management)\n",
      "\t+ Education (e.g., adaptive learning, student assessment)\n",
      "\n",
      "**Challenges Ahead**\n",
      "\n",
      "* Addressing AI's limitations and biases\n",
      "* Ensuring transparency, explainability, and accountability in AI decision-making\n",
      "* Balancing the benefits of AI with concerns about job displacement and social impact\n",
      "\n",
      "The history of AI is a story of innovation, perseverance, and collaboration. As we continue to push the boundaries of what is possible, it's essential to acknowledge the past while shaping the future of this exciting field!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What a fascinating topic!\\n\\nThe history of Artificial Intelligence (AI) dates back to ancient civilizations, but I\\'ll focus on the modern era. Here\\'s an overview:\\n\\n**Early Beginnings**\\n\\n* 1950s: The Dartmouth Summer Research Project on Artificial Intelligence (DSRPAI) was established, marking the birth of AI as a distinct field. John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon were among the key figures.\\n* 1951: Alan Turing published \"Computing Machinery and Intelligence,\" a paper that proposed the Turing Test, a measure of a machine\\'s ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human.\\n\\n**Golden Age**\\n\\n* 1960s: The term \"Artificial Intelligence\" was coined by John McCarthy. This era saw significant advancements in AI research:\\n\\t+ Rule-based systems (e.g., Mycin, a medical diagnosis program)\\n\\t+ Logic programming (e.g., Prolog)\\n\\t+ Machine learning (e.g., decision trees)\\n\\n**AI Winter**\\n\\n* 1970s-1980s: Despite the initial excitement and progress, AI research faced significant challenges and funding cuts. This period became known as the \"AI winter.\" Reasons included:\\n\\t+ Limited understanding of human intelligence\\n\\t+ Lack of computational power and data storage\\n\\t+ High expectations vs. slow progress\\n\\n**Resurgence**\\n\\n* 1980s-1990s: AI research experienced a resurgence, driven by advances in computing power, memory, and data storage. This led to the development of:\\n\\t+ Expert systems (e.g., MYCIN, R1)\\n\\t+ Knowledge representation languages (e.g., frames, semantic networks)\\n\\n**Modern Era**\\n\\n* 2000s: AI research accelerated with the rise of:\\n\\t+ Machine learning (ML) and deep learning (DL)\\n\\t+ Natural Language Processing (NLP) and computer vision\\n\\t+ Big data and the Internet of Things (IoT)\\n\\t+ Open-source frameworks like TensorFlow, PyTorch, and Keras\\n\\n**Current State**\\n\\n* AI is now a rapidly growing field, with applications in:\\n\\t+ Healthcare (e.g., medical diagnosis, personalized medicine)\\n\\t+ Finance (e.g., risk analysis, portfolio optimization)\\n\\t+ Transportation (e.g., autonomous vehicles, traffic management)\\n\\t+ Education (e.g., adaptive learning, student assessment)\\n\\n**Challenges Ahead**\\n\\n* Addressing AI\\'s limitations and biases\\n* Ensuring transparency, explainability, and accountability in AI decision-making\\n* Balancing the benefits of AI with concerns about job displacement and social impact\\n\\nThe history of AI is a story of innovation, perseverance, and collaboration. As we continue to push the boundaries of what is possible, it\\'s essential to acknowledge the past while shaping the future of this exciting field!'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3:latest\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "llm(\"Tell me about the history of AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"llama3:latest\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_grader= prompt| llm| JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"yes\"}{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "print(retrival_grader.invoke({\"question\":question, \"document\":doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where to find iphone?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\": \"no\"}{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "print(retrival_grader.invoke({\"question\":question, \"document\":doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks.\n",
    "    Use the following piece of retrieved context to answer the question.  If you do not know the answer, just say that you do not know.\n",
    "    Use three sentens maximum and keep the answer short.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: \\n\\n {question} \\n\\n\n",
    "    Conext: {context} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"llama3:latest\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'}),\n",
       " Document(page_content='A: It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly.\\n\\n### Q: Can I reduce LLM costs without compromising performance?\\n\\nA: Yes, it is possible to reduce LLM costs without compromising performance. By carefully selecting the right models for specific tasks, optimizing agent memory, and using techniques like LLM Lingua, you can achieve cost savings while maintaining high performance and user experience.\\n\\n### Q: Are there any other cost optimization methods for LLM that I should be aware of?\\n\\nA: While the methods mentioned in this article are effective for reducing LLM costs, there may be other innovative approaches and techniques available. Stay updated with the latest research and developments in the field to discover new cost optimization methods.\\n\\nRelated articles\\n----------------\\n\\n[Browse all articles](/)', metadata={'description': 'AI Agent, Real cost of LLM', 'ogDescription': 'AI Agent, Real cost of LLM', 'ogImage': 'https://assets-global.website-files.com/647a9c825f099afe643b5c78/65bedbda9969f366afdfad25_tiktok%20(1).jpg', 'ogTitle': 'How to reduce 78%+ of LLM Cost', 'sourceURL': 'https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost', 'title': 'How to reduce 78%+ of LLM Cost'})]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain= prompt| llm| StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to reduce llm cost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriver.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly. You can also carefully select the right models for specific tasks, optimize agent memory, and use techniques like LLM Lingua to achieve cost savings while maintaining high performance and user experience."
     ]
    }
   ],
   "source": [
    "generation= rag_chain.invoke({\"context\":docs, \"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is recommended to monitor and optimize your LLM costs regularly, especially as your usage and user base grow. Keep track of cost patterns, identify areas for improvement, and implement cost optimization strategies accordingly. You can also carefully select the right models for specific tasks, optimize agent memory, and use techniques like LLM Lingua to achieve cost savings while maintaining high performance and user experience.\n"
     ]
    }
   ],
   "source": [
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
